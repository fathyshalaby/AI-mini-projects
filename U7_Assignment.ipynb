{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <p style=\"text-align: left;\">Name</p>               | Matr.Nr. | <p style=\"text-align: right;\">Date</p> |\n",
    "| --------------------------------------------------- | -------- | ------------------------------------- |\n",
    "| <p style=\"text-align: left\">Fathy Shalaby</p> | 11701175 | 14.05.2020                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 7 -- Introduction to Natural Language Processing II </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors</b>: Rekabsaz, Brandstetter <br>\n",
    "<b>Date</b>: 11-05-2020\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0\n",
    "\n",
    "- Import the same modules as discussed in the lecture notebook.\n",
    "- Check if your model versions are correct.\n",
    "- Use your GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your imports go here\n",
    "import u7_utils as u7\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.7 (✓)\n",
      "Installed numpy version: 1.18.1 (✓)\n",
      "Installed matplotlib version: 3.1.3 (✓)\n",
      "Installed PyTorch version: 1.5.0 (✓)\n"
     ]
    }
   ],
   "source": [
    "# your imports go here\n",
    "u7.check_module_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# your cuda check goes here\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(208,90,80)\">ABOUT THIS NOTEBOOK</h1>\n",
    "<span style=\"color:rgb(208,90,80)\">In this notebook you should solve a small task on your one. <br><br> The goal is to train an LSTM network with a different number of hidden cells on the Penn Treebank dataset. You should decide on the validation dataset which model works best and then try it on the test dataset. This is a first example of a hyperparameter search. <br> We only evaluate how you build this hyperparameter search.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Defining hyper-parameters</h3>\n",
    "In contrast to the lecture notebook we do not set the parameter <i> nhid </i>. This is the hyperparameter which we will later use for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'resources/penn/'\n",
    "emsize = 200 # size of word embeddings\n",
    "lr = 20 # initial learning rate\n",
    "clipping = 0.25 # gradient clipping\n",
    "epochs = 3 # upper epoch limit\n",
    "train_batch_size = 10 # batch size for training\n",
    "eval_batch_size = 5 # batch size for elidation/test\n",
    "max_seq_len = 35 # sequence length\n",
    "seed = 1111 # random seed to facilitate reproducability\n",
    "print_interval = 1000 # report interval\n",
    "save_path = 'model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6fd407b2b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Data & dictionary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dictionary 10001\n",
      "Train data: number of tokens 929589\n",
      "Validation data: number of tokens 73760\n",
      "Test data: number of tokens 82430\n"
     ]
    }
   ],
   "source": [
    "train_corpus = u7.Corpus(os.path.join(data_path, 'train.txt'))\n",
    "valid_corpus = u7.Corpus(os.path.join(data_path, 'valid.txt'))\n",
    "test_corpus = u7.Corpus(os.path.join(data_path, 'test.txt'))\n",
    "\n",
    "dictionary = u7.Dictionary()\n",
    "train_corpus.fill_dictionary(dictionary)\n",
    "ntokens = len(dictionary)\n",
    "print (f'Number of tokens in dictionary {ntokens}')\n",
    "\n",
    "train_data = train_corpus.words_to_ids(dictionary)\n",
    "print (f'Train data: number of tokens {len(train_data)}')\n",
    "\n",
    "valid_data = valid_corpus.words_to_ids(dictionary)\n",
    "print (f'Validation data: number of tokens {len(valid_data)}')\n",
    "\n",
    "test_data = test_corpus.words_to_ids(dictionary)\n",
    "print (f'Test data: number of tokens {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batchified data shape: torch.Size([92958, 10])\n",
      "Validation batchified data shape: torch.Size([14752, 5])\n",
      "Test batchified data shape: torch.Size([16486, 5])\n"
     ]
    }
   ],
   "source": [
    "train_data_batches = u7.batchify(train_data, train_batch_size, device)\n",
    "print (f'Train batchified data shape: {train_data_batches.shape}')\n",
    "\n",
    "val_data_batches = u7.batchify(valid_data, eval_batch_size, device)\n",
    "print (f'Validation batchified data shape: {val_data_batches.shape}')\n",
    "\n",
    "test_data_batches = u7.batchify(test_data, eval_batch_size, device)\n",
    "print (f'Test batchified data shape: {test_data_batches.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Training</h3>\n",
    "Nothing to do here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, dictionary: u7.Dictionary,\n",
    "          max_seq_len: int, train_batch_size: int, \n",
    "          train_data_batches, optimizer: torch.optim.Optimizer,\n",
    "          criterion: torch.nn, clipping: int, learning_rate: int,\n",
    "          print_interval: int, epoch: int):\n",
    "    \"\"\"\n",
    "    Function to train the model. \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = model.init_hidden(train_batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data_batches.size(0) - 1, max_seq_len)):\n",
    "        data, targets = u7.get_batch(train_data_batches, i, max_seq_len)\n",
    "\n",
    "        # forward pass\n",
    "        model.zero_grad()\n",
    "        start_hidden = u7.repackage_hidden(start_hidden)\n",
    "        output, last_hidden = model(data, start_hidden)\n",
    "\n",
    "        # loss computation & backward pass\n",
    "        output = output.view(-1, ntokens)\n",
    "        loss = criterion(output, targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        start_hidden = last_hidden\n",
    "        # clipping gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % print_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / print_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'| epoch {epoch :3d} | {batch :5d}/{int(len(train_data_batches)/max_seq_len) :5d} batches' \n",
    "                  f'| lr {learning_rate :02.2f} | ms/batch {elapsed * 1000 / print_interval :5.2f} |'\n",
    "                  f'loss {cur_loss :5.2f} | perplexity {math.exp(cur_loss) :8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid):\n",
    "        super(LM_LSTMModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.nhid = nhid\n",
    "        \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(1, bsz, self.nhid),\n",
    "                weight.new_zeros(1, bsz, self.nhid))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.encoder(input)\n",
    "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        decoded = self.decoder(hiddens)\n",
    "        return F.log_softmax(decoded, dim=-1), last_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "- Train the model for three epochs and validate after each epoch. Repeat this procedure with different number of LSTM cells (the <i> nhid </i> parameter in the lecture notebook). Save the best models for the different runs.\n",
    "- What is the best model? You can use the suggested parameter values but you can try different values too if wanted. Please note that for larger number of LSTM cells the training might be pretty time-consuming.\n",
    "- Load the best model and evaluate it on the test dataset.\n",
    "- NOTA BENE: use the Adam optimizer to get better performance <code> optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5)</code>, instead of SGD as done in the lecture (you can check for it in earlier notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhid= [8, 16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 45.80 |loss  6.33 | perplexity   558.46\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 51.41 |loss  5.91 | perplexity   369.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 140.45s| valid loss  5.83 | valid perplexity   339.14\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathy/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LM_LSTMModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 60.83 |loss  5.77 | perplexity   320.15\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 56.71 |loss  5.72 | perplexity   304.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 158.39s| valid loss  5.73 | valid perplexity   306.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 56.67 |loss  5.68 | perplexity   293.36\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 55.87 |loss  5.66 | perplexity   285.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 155.66s| valid loss  5.68 | valid perplexity   292.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 66.57 |loss  6.09 | perplexity   442.76\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 60.31 |loss  5.71 | perplexity   300.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 167.85s| valid loss  5.63 | valid perplexity   279.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 77.04 |loss  5.55 | perplexity   257.91\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 63.76 |loss  5.49 | perplexity   242.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 187.88s| valid loss  5.52 | valid perplexity   248.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 60.31 |loss  5.44 | perplexity   230.15\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 65.89 |loss  5.41 | perplexity   224.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 180.70s| valid loss  5.46 | valid perplexity   236.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 81.20 |loss  5.88 | perplexity   357.06\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 80.91 |loss  5.48 | perplexity   239.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 215.60s| valid loss  5.43 | valid perplexity   228.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 78.86 |loss  5.33 | perplexity   206.79\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 72.46 |loss  5.29 | perplexity   197.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 204.52s| valid loss  5.34 | valid perplexity   208.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 75.21 |loss  5.23 | perplexity   187.56\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 74.71 |loss  5.21 | perplexity   183.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 210.06s| valid loss  5.28 | valid perplexity   196.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 98.96 |loss  5.76 | perplexity   316.10\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 107.07 |loss  5.35 | perplexity   210.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 285.47s| valid loss  5.31 | valid perplexity   203.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 103.54 |loss  5.20 | perplexity   180.60\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 103.58 |loss  5.16 | perplexity   173.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 270.96s| valid loss  5.22 | valid perplexity   184.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 90.77 |loss  5.10 | perplexity   163.62\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 91.96 |loss  5.08 | perplexity   161.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 248.44s| valid loss  5.15 | valid perplexity   171.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 124.99 |loss  5.66 | perplexity   286.33\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 129.84 |loss  5.26 | perplexity   192.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 341.13s| valid loss  5.22 | valid perplexity   184.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 124.46 |loss  5.09 | perplexity   163.11\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 124.36 |loss  5.05 | perplexity   156.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 338.61s| valid loss  5.13 | valid perplexity   168.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 127.50 |loss  4.99 | perplexity   146.73\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 125.76 |loss  4.98 | perplexity   145.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 336.82s| valid loss  5.06 | valid perplexity   158.11\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for cells in nhid:\n",
    "    # your code goes here\n",
    "    model = LM_LSTMModel(ntokens, emsize, cells).to(device)\n",
    "    best_val_loss = None\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model=model, dictionary=dictionary, max_seq_len=max_seq_len, train_batch_size=train_batch_size, train_data_batches=train_data_batches, \n",
    "              criterion=criterion, clipping=clipping, learning_rate=lr, print_interval=print_interval, epoch=epoch,optimizer=optimizer)\n",
    "        val_loss = u7.evaluate(model, dictionary, max_seq_len, \n",
    "                               eval_batch_size, val_data_batches, criterion)\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch :3d} | time: {time.time() - epoch_start_time :5.2f}s' \n",
    "              f'| valid loss {val_loss :5.2f} | valid perplexity {math.exp(val_loss):8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.03 | test perplexity 152.18\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "with open(save_path, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    \n",
    "test_loss = u7.evaluate(model, dictionary, max_seq_len, \n",
    "                           eval_batch_size, test_data_batches, criterion)\n",
    "print('=' * 89)\n",
    "#print('| End of training | test loss {:5.2f} | test perplexity {:8.2f}'.format(\n",
    "#    test_loss, math.exp(test_loss)))\n",
    "print(f'| End of training | test loss {test_loss :5.2f} | test perplexity {math.exp(test_loss) :5.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "- Count the parameters of the best model. How many parameters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters 3459289\n"
     ]
    }
   ],
   "source": [
    "print('Number of Parameters',sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
