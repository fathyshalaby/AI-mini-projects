{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "U9_Assignment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj5ub2MlgwGB",
        "colab_type": "text"
      },
      "source": [
        "| <p style=\"text-align: left;\">Name</p>               | Matr.Nr. | <p style=\"text-align: right;\">Date</p> |\n",
        "| --------------------------------------------------- | -------- | ------------------------------------- |\n",
        "| <p style=\"text-align: left\">Fathy Shalaby</p> | 11701175 | 10.06.2020                            |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH6TzipJgwGE",
        "colab_type": "text"
      },
      "source": [
        "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
        "<h2 style=\"color:rgb(0,120,170)\">Unit 9 (Assignment) -- Introduction to Reinforcement Learning -- Part II </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrQCX_MpgwGG",
        "colab_type": "text"
      },
      "source": [
        "<b>Authors</b>: Brandstetter, Schäfl <br>\n",
        "<b>Date</b>: 08-06-2020\n",
        "\n",
        "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies \n",
        "to all code within this file.\n",
        "\n",
        "<b>Copyright statement</b>: <br>\n",
        "This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJfLk8LtgwGI",
        "colab_type": "text"
      },
      "source": [
        "<h2>Exercise 0</h2>\n",
        "\n",
        "- Import the same modules as discussed in the lecture notebook.\n",
        "- Check if your model versions are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N3LqwjniFyT",
        "colab_type": "code",
        "outputId": "7979fa77-7433-4e4f-f189-a574972ae3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf_s8eHJiGcD",
        "colab_type": "code",
        "outputId": "10ce0040-cb32-4a34-b9d3-ecbd0e24d38b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive/My Drive/as9"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/as9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcoMAhkrgwGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your imports go here\n",
        "import u9_utils as u9\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from typing import Any, Dict, Tuple\n",
        "from gym.envs.toy_text import FrozenLakeEnv\n",
        "\n",
        "# Set Seaborn plotting style.\n",
        "sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHYkmCcKgwGY",
        "colab_type": "code",
        "outputId": "b66100d1-ecac-4bb5-a671-b35b590223d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# your modul version check goes here\n",
        "u9.check_module_versions()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installed Python version: 3.6 (✗)\n",
            "Installed matplotlib version: 3.2.1 (✓)\n",
            "Installed Pandas version: 1.0.4 (✓)\n",
            "Installed Seaborn version: 0.10.1 (✓)\n",
            "Installed OpenAI Gym version: 0.17.2 (✓)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWVLQUN-gwGe",
        "colab_type": "text"
      },
      "source": [
        "All exercises in this assignment are referring to the <i>FrozenLake-v0</i> environment of <a href=\"https://gym.openai.com\"><i>OpenAI Gym</i></a>. This environment is descibed according to its official <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">OpenAI Gym website</a> as follows:<br>\n",
        "<cite>Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.</cite>\n",
        "\n",
        "\n",
        "There are <i>four</i> types of surfaces described in this environment:\n",
        "<ul>\n",
        "    <li><code>S</code> $\\rightarrow$ starting point (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
        "    <li><code>F</code> $\\rightarrow$ frozen surface (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
        "    <li><code>H</code> $\\rightarrow$ hole (<span style=\"color:rgb(255,0,0)\"><i>fall to your doom</i></span>)</li>\n",
        "    <li><code>G</code> $\\rightarrow$ goal (<span style=\"color:rgb(255,0,255)\"><i>frisbee location</i></span>)</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "If not already done, more information on how to <i>install</i> and <i>import</i> the <code>gym</code> module is available in the lecture's notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBr19rJegwGf",
        "colab_type": "text"
      },
      "source": [
        "<h3 style=\"color:rgb(0,120,170)\">States and actions</h3>\n",
        "Experiment with the <i>FrozenLake-v0</i> environment as discussed during the lecture and explained in the accompanying notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4ZqIx9IgwGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lake_environment = FrozenLakeEnv()\n",
        "u9.set_seed(environment=lake_environment, seed=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZXsp5LpgwGk",
        "colab_type": "code",
        "outputId": "c23a6a34-ab4d-4da7-90b2-a0d0784ea5ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "lake_environment.render(mode=r'human')\n",
        "current_state_id = lake_environment.s\n",
        "print(f'\\nCurrent state ID: {current_state_id}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Current state ID: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGIzM8zWgwGn",
        "colab_type": "text"
      },
      "source": [
        "The current position of the <i>disc retrieving</i> entity is displayed as a filled <span style=\"color:rgb(255,0,0)\"><i>red</i></span> rectangle.\n",
        "\n",
        "As we want to tackle this problem using our renowned <i>random search</i> approach, we have to analyse its applicability beforehand. Hence, the number of possible <i>actions</i> and <i>states</i> is of utter importance, as we don't want to get lost in the depth of combinatorial explosion.\n",
        "<ul>\n",
        "    <li>Query the amount of <i>actions</i> using the appropriate peoperty of the lake environment.</li>\n",
        "    <li>Query the amount of <i>states</i> using the appropriate property of the lake environment.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aLzhboOgwGn",
        "colab_type": "code",
        "outputId": "a9817adc-5259-4335-9f04-1ab3fe1a3c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_actions = lake_environment.action_space.n\n",
        "num_states = lake_environment.observation_space.n\n",
        "print(f'The FrozenLake-v0 environment comprises <{num_actions}> actions and <{num_states}> states.')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The FrozenLake-v0 environment comprises <4> actions and <16> states.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk_bBTtogwGq",
        "colab_type": "text"
      },
      "source": [
        "<h2>Exercise 1</h2>\n",
        "\n",
        "- Create a q_table for the frozen lake environment.\n",
        "- Apply $Q$-learning as it was done in the lecture to solve the environment.\n",
        "- Test the learned policy and animate one (or more) exemplary episode.\n",
        "- What do you observe? Does the agent learn anything useful? Discuss if something strange happens. Hint: print the q_table during training to better understand what is going on during learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5UIM9WLgwGq",
        "colab_type": "code",
        "outputId": "e3485053-a23a-4b7c-c8fd-9cafa6e2dc85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# q_table\n",
        "q_table = np.zeros([lake_environment.observation_space.n, lake_environment.action_space.n])\n",
        "q_table"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMLHx0W2gwGs",
        "colab_type": "code",
        "outputId": "38259f28-fbd7-4b39-c5e4-02011f53b382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# shape of q_table\n",
        "q_table.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBQi8FwBgwGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_q_learning(environment: lake_environment, alpha: float = 0.1):\n",
        "    \"\"\"\n",
        "    Solve lake_environment by applying Q learning.\n",
        "    \"\"\"\n",
        "    for i in range(1, 10001):\n",
        "        # your code goes here\n",
        "        state = environment.reset()\n",
        "        done = False\n",
        "    \n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state]) \n",
        "            next_state, reward, done, info = environment.step(action) \n",
        "            old_value = q_table[state, action]\n",
        "            next_max = np.max(q_table[next_state])\n",
        "            new_value = (1 - alpha) * old_value + alpha * (reward + next_max)\n",
        "            q_table[state, action] = new_value\n",
        "\n",
        "            state = next_state\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Episode: {i}\")\n",
        "            print(q_table)   \n",
        "\n",
        "    print(\"Training finished.\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWe-3pfSgwGw",
        "colab_type": "code",
        "outputId": "6148fa6d-57bd-4392-c0cf-8666c2894b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "%%time\n",
        "from IPython.display import clear_output\n",
        "# train your agent\n",
        "alpha = 0.1\n",
        "apply_q_learning(lake_environment, alpha)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 10000\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 4.67 s, sys: 608 ms, total: 5.28 s\n",
            "Wall time: 4.75 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFd4FCMRgwGx",
        "colab_type": "code",
        "outputId": "e783b919-b1fb-4458-9ae4-0c559ffede7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "total_epochs, total_dives = 0, 0\n",
        "episodes = 10000\n",
        "\n",
        "captured_frames = [[] for _ in range(episodes)]\n",
        "\n",
        "for episode in range(episodes):\n",
        "    # test your method\n",
        "    state = lake_environment.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = lake_environment.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "        \n",
        "        # Save rendering o f current state.\n",
        "        captured_frames[episode].append({\n",
        "            r'frame': lake_environment.render(mode=r'ansi'),\n",
        "            r'state': state,\n",
        "            r'action': action,\n",
        "            r'reward': reward\n",
        "        })\n",
        "\n",
        "    total_dives += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average dives per episode: {total_dives / episodes}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 17.19\n",
            "Average dives per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hv-nDgrgwG0",
        "colab_type": "code",
        "outputId": "cbed0856-3443-4146-8cf4-713f3e065cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# animate some of the results\n",
        "u9.animate_environment_search(frames=captured_frames[12], verbose=True, delay=0.4)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n",
            "Step No.: 5\n",
            "State ID: 12\n",
            "Action ID: 0\n",
            "Reward: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te4FkXrogwG1",
        "colab_type": "text"
      },
      "source": [
        "<h2>Exercise 2</h2>\n",
        "Very likely your training in Exercise 1 was not successful. Try to add exploration to your algorithm (you might have to write a new function):\n",
        "<li><code>I</code> $\\rightarrow$ Throw a random uniform number between 0 and 1. \n",
        "<li><code>II</code> $\\rightarrow$ If the number is smaller than 0.1, sample a random action.\n",
        "<li><code>III</code> $\\rightarrow$ Choose your action as usual.   \n",
        "    \n",
        "- Apply the modified $Q$-learning again to solve the environment.\n",
        "- Test the learned policy and animate one (or more) exemplary episode.\n",
        "- What do you observe? Does the agent learn now?."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq225iIOgwG3",
        "colab_type": "code",
        "outputId": "1d019d7b-2c4b-4317-bc7a-1d77f38bd286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# q_table\n",
        "q_table = np.zeros([lake_environment.observation_space.n, lake_environment.action_space.n])\n",
        "q_table"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlBrA_GtgwG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_q_learning_exploration(environment: lake_environment, alpha: float = 0.1):\n",
        "    \"\"\"\n",
        "    Solve lake_environment by applying Q learning and exploration.\n",
        "    \"\"\"\n",
        "    for i in range(1, 10001):\n",
        "        # your code goes here\n",
        "        # Throw a random uniform number between 0 and 1\n",
        "        # If the number is smaller than 0.1, sample a random action.\n",
        "        # Choose your action as usual.\n",
        "        state = environment.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            num = np.random.uniform(0,1)\n",
        "            if num < alpha:\n",
        "              action = lake_environment.action_space.sample()\n",
        "            else:\n",
        "              action = np.argmax(q_table[state]) \n",
        "            next_state, reward, done, info = environment.step(action) \n",
        "            old_value = q_table[state, action]\n",
        "            next_max = np.max(q_table[next_state])\n",
        "            new_value = (1 - alpha) * old_value + alpha * (reward + next_max)\n",
        "            q_table[state, action] = new_value\n",
        "\n",
        "            state = next_state\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Episode: {i}\")\n",
        "            print(q_table)   \n",
        "    print(\"Training finished.\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-S7pG8JgwG6",
        "colab_type": "code",
        "outputId": "630a10af-d0a2-4c83-a032-abe2aac7b617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "%%time\n",
        "from IPython.display import clear_output\n",
        "# train your agent\n",
        "alpha = 0.1\n",
        "apply_q_learning_exploration(lake_environment, alpha)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 10000\n",
            "[[0.77222677 0.75566608 0.76416355 0.75991826]\n",
            " [0.48776354 0.31521277 0.43941593 0.69773066]\n",
            " [0.54905981 0.42002365 0.39975704 0.5184715 ]\n",
            " [0.23150219 0.07878616 0.09765571 0.07743555]\n",
            " [0.77732367 0.61322777 0.56736619 0.52477322]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.32684521 0.21219287 0.37495681 0.15066474]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.53642365 0.56613119 0.36771388 0.78449932]\n",
            " [0.7103599  0.78899253 0.52449693 0.44644416]\n",
            " [0.73175767 0.51156238 0.59075405 0.36403519]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.61700706 0.70163724 0.84566247 0.52615417]\n",
            " [0.79876141 0.93116259 0.88754576 0.84422637]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 10.1 s, sys: 2.57 s, total: 12.6 s\n",
            "Wall time: 9.71 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxHgP-jrgwG7",
        "colab_type": "code",
        "outputId": "1d20445a-eda4-4dd9-8e40-069b38f8be8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "total_epochs, total_dives = 0, 0\n",
        "episodes = 10000\n",
        "\n",
        "captured_frames = [[] for _ in range(episodes)]\n",
        "\n",
        "for episode in range(episodes):\n",
        "    # test your method\n",
        "    state = lake_environment.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = lake_environment.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "        \n",
        "        # Save rendering of current state.\n",
        "        captured_frames[episode].append({\n",
        "            r'frame': lake_environment.render(mode=r'ansi'),\n",
        "            r'state': state,\n",
        "            r'action': action,\n",
        "            r'reward': reward\n",
        "        })\n",
        "\n",
        "    total_dives += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average dives per episode: {total_dives / episodes}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 10000 episodes:\n",
            "Average timesteps per episode: 43.5011\n",
            "Average dives per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLgeH9PSgwG9",
        "colab_type": "code",
        "outputId": "cf9a7e88-eb1b-4f66-e009-b1c33ce215af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "u9.animate_environment_search(frames=captured_frames[4], verbose=True, delay=0.1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Step No.: 102\n",
            "State ID: 15\n",
            "Action ID: 1\n",
            "Reward: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}